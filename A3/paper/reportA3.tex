\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{A3 homework submission \\ Team: the gurecki \\ Deep Learning 2015, Spring}


\author{
David Halpern\\
Department of Psychology\\
New York University\\
\texttt{david.halpern@nyu.edu} \\
\And
Anselm Rothe\\
Department of Psychology\\
New York University\\
\texttt{ar3918@nyu.edu} \\
\AND
Alex Rich\\
Department of Psychology\\
New York University\\
\texttt{asr443@nyu.edu} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % camera-ready version

\begin{document}


\maketitle


\section{Architecture}

\subsection{Model 1}

We extended the baseline model to use Glove 100 (we couldn't solve memory errors that we got for larger Glove files).
We added a hidden layer and a dropout layer.



\subsection{Model 2}


\section{Learning Techniques}

We did not invest in data augmentations. 

\section{Training Procedure}

\subsection{Model 1}

The model was trained in 10 epochs using stochastic gradient descent with a batch size of 32, a learning rate of .1, a learning rate decay of 1e-7, a momentum of .9, and no weight decay.

Performance

\subsection{Model 2}

-- Alex


For both models we used the typical classNLLcriterion as the loss function. We used a validation set of 13K documents (10\%) per class of the original 130K labeled training examples per class.
The optimization procedure was run over the remaining 117K (90\%) training documents per class.


\section{Experiments}

These experiments did not help to boost the performance and thus where not included in our final submission.

\subsection{Continuous output variable}
We attempted to use uniform prescription to train our model using the unlabeled data. In order to 
do this, we used KL Distance as our loss function so that the target for the unlabeled data could be
a uniform distribution on all of the categories. The targets for the labeled data were a delta function
with all probability mass on the true category. We tried using various amounts of unlabeled data from
500 to all 100000 but this did not seem to improve performance


\subsubsection*{References}

\small{
[1] Dosovitskiy, A., Springenberg, J. T., Riedmiller, M., \& Brox, T. (2014). Discriminative unsupervised feature learning with convolutional neural networks. {\it Advances in Neural Information Processing Systems}, pp. 766-774.

[2] Xiang Zhang (2015). {\it How to Train STL-10 Well}. http://goo.gl/xJGvyH
}


\end{document}
