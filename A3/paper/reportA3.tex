\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{A3 homework submission \\ Team: the gurecki \\ Deep Learning 2015, Spring}


\author{
David Halpern\\
Department of Psychology\\
New York University\\
\texttt{david.halpern@nyu.edu} \\
\And
Anselm Rothe\\
Department of Psychology\\
New York University\\
\texttt{ar3918@nyu.edu} \\
\AND
Alex Rich\\
Department of Psychology\\
New York University\\
\texttt{asr443@nyu.edu} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % camera-ready version

\begin{document}


\maketitle


\section{Architecture}

\subsection{Model 1}

We extended the baseline model to use Glove 100 (we couldn't solve memory errors that we got for larger Glove files).
We added a hidden layer with 500 units.

\begin{verbatim}
(1): nn.Reshape(100)
(2): nn.ReLU
(3): nn.Linear(100 -> 500)
(4): nn.Dropout
(5): nn.ReLU
(6): nn.Linear(500 -> 5)
(7): nn.LogSoftMax
\end{verbatim}



\subsection{Model 2}


\section{Learning Techniques}

Model 1 used dropout (.5). We did not invest in data augmentations. 

\section{Training Procedure}

For both models we used the typical classNLLcriterion as the loss function. We used a validation set of 13K documents (10\%) per class of the original 130K labeled training examples per class.
The optimization procedure was run over the remaining 117K (90\%) training documents per class.

\begin{figure}
	\includegraphics[width=0.8\textwidth]{Training}
\end{figure}

\subsection{Model 1}

The model was trained in 10 epochs using stochastic gradient descent with a batch size of 32, a learning rate of .1, a learning rate decay of 1e-7, a momentum of .9, and no weight decay.

\subsection{Model 2}

-- Alex


\section{Experiments}

These experiments did not help to boost the performance and thus where not included in our final submission.

\subsection{Continuous output variable}
We explored the idea that a single continuous output variable might be better than the five seperate classes because the star ratings are rank-ordered. 
For example, if the model predicts high likelihoods for class 2 and 4, then class 3 might actually be its best guess.
\begin{verbatim}
(6): nn.Linear(500 -> 4)
(7): nn.Sigmoid
(8): nn.Sum
(9): nn.Add

criterion = nn.MSECriterion()
\end{verbatim}
Layer 8 is taking the sum of 4 sigmoids which results in a number between 0 and 4 to which layer 9 adds a bias to bring the range close to 1--5.
This structure gave better results than a simple unconstrained continuous output 
\begin{verbatim}
(6): nn.Linear(500 -> 1)
\end{verbatim}
but did not reach the performance of the 5-classes output that we eventually used for Model 1.


\subsubsection*{References}

\small{
% [1] Dosovitskiy, A., Springenberg, J. T., Riedmiller, M., \& Brox, T. (2014). Discriminative unsupervised feature learning with convolutional neural networks. {\it Advances in Neural Information Processing Systems}, pp. 766-774.

% [2] Xiang Zhang (2015). {\it How to Train STL-10 Well}. http://goo.gl/xJGvyH
}


\end{document}
